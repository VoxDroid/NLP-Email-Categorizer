{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **⚙ Text Classification Pipeline for Email Subjects by VoxDroid ⚙**\n",
        "---\n",
        "This notebook provides a comprehensive text classification pipeline specifically designed for analyzing and categorizing email subjects. The step-by-step process includes data loading, text preprocessing, data splitting, feature extraction using CountVectorizer, model training with Multinomial Naive Bayes, model evaluation, saving the trained model, and making predictions on new data. The implementation leverages popular Python libraries such as pandas, scikit-learn, joblib, and NLTK.\n",
        "\n",
        "Designed for individuals exploring natural language processing (NLP) or aiming to build practical email categorization systems, this tutorial demonstrates how to preprocess data, train a Multinomial Naive Bayes model, evaluate its performance, and make predictions on new text."
      ],
      "metadata": {
        "id": "lNWgt3kJPPMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd0F9z-sd4Bn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 1: Data Preparation ✔\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "file_name = \"your_tsv_file_here.tsv\" #@param {type:\"string\"}\n",
        "data = pd.read_csv(file_name, sep='\\t')\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Text Preprocessing ✔\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Subject'] = data['Subject'].apply(preprocess_text)\n",
        "\n",
        "data['Subject'].head()"
      ],
      "metadata": {
        "id": "9xA9gC2uf7bv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Splitting the Data ✔\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Subject'], data['Category'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "LIuAsRlMf8Kd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Handling Missing Values ✔\n",
        "\n",
        "X_train = X_train.fillna('')\n",
        "y_train = y_train.dropna()\n",
        "X_train = X_train[:len(y_train)]"
      ],
      "metadata": {
        "id": "IT9GGoSA0pHG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Feature Extraction with CountVectorizer ✔\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "RFmjl2syf8NO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6: Model Training with Multinomial Naive Bayes ✔\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vectorized, y_train)"
      ],
      "metadata": {
        "id": "9p5K0Nfrf8P2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7: Model Evaluation ✔\n",
        "y_test_str = y_test.astype(str)\n",
        "predictions = model.predict(X_test_vectorized)\n",
        "predictions_str = predictions.astype(str)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test_str, predictions_str)}\")\n",
        "print(classification_report(y_test_str, predictions_str))"
      ],
      "metadata": {
        "id": "qnYnCN_QgKrt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8: Save the Trained Model and Create a Zip File ✔\n",
        "import zipfile\n",
        "\n",
        "joblib.dump(model, 'esub_model.joblib')\n",
        "\n",
        "with zipfile.ZipFile('model.zip', 'w') as zipf:\n",
        "    zipf.write('esub_model.joblib')"
      ],
      "metadata": {
        "id": "10fCM1SSf8St",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 9: Load the Trained Model and Make a Prediction ✔\n",
        "loaded_model = joblib.load('esub_model.joblib')\n",
        "\n",
        "Subject = \"Your email subject text here\" #@param {type:\"string\"}\n",
        "\n",
        "preprocessed_text = preprocess_text(Subject)\n",
        "\n",
        "text_vectorized = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "prediction = loaded_model.predict(text_vectorized)\n",
        "\n",
        "print(f\"Predicted Category: {prediction[0]}\")"
      ],
      "metadata": {
        "id": "QXxU0LHxgPDv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}